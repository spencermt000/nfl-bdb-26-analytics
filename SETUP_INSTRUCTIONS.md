# NFL Big Data Bowl 2026 - Pipeline Orchestration Setup

## Overview

This setup provides **two ways** to run your data processing pipeline:

1. **Standalone Python** (Recommended for quick runs)
2. **Airflow + Docker** (Recommended for production, scheduling, monitoring)

---

## Quick Start - Standalone Mode (No Docker)

### Prerequisites
```bash
# Install Python dependencies
pip install -r requirements.txt
```

### Run Pipeline
```bash
# Run entire pipeline
python run_it.py --mode standalone

# Pipeline will execute:
# 1. Check prerequisites
# 2. Run dataframe_a_v2.py
# 3. Run dataframe_b_v3.py  
# 4. Run dataframe_d.py
# 5. Run dataframe_c_v3.py
# 6. Generate summary report
```

**Advantages:**
- âœ… Simple, no Docker required
- âœ… Fast setup
- âœ… Easy debugging
- âœ… Good for development

**Disadvantages:**
- âŒ No task scheduling
- âŒ No retry logic
- âŒ No monitoring UI
- âŒ No parallel execution

---

## Advanced Setup - Airflow + Docker

### Prerequisites

1. **Docker & Docker Compose installed**
   ```bash
   # Check installation
   docker --version
   docker-compose --version
   ```

2. **Create required directories**
   ```bash
   mkdir -p dags logs plugins outputs
   ```

3. **Set up environment variables**
   ```bash
   # Copy example file
   cp .env.example .env
   
   # Edit .env and set your user ID
   # Run: id -u
   # Then update AIRFLOW_UID in .env
   ```

4. **Copy DAG file**
   ```bash
   # Copy run_it.py to dags folder
   cp run_it.py dags/
   ```

### Start Airflow

```bash
# Initialize Airflow (first time only)
docker-compose up airflow-init

# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f
```

### Access Airflow UI

1. Open browser: **http://localhost:8080**
2. Login:
   - Username: `airflow`
   - Password: `airflow`
3. Find DAG: `nfl_bdb_data_pipeline`
4. Click "Trigger DAG" to run pipeline

### Monitor Pipeline

The Airflow UI shows:
- âœ… Task status (success/failure/running)
- â±ï¸ Task duration
- ğŸ“Š Logs for each task
- ğŸ”„ Retry attempts
- ğŸ“ˆ Historical runs

### Stop Airflow

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```

**Advantages:**
- âœ… Task scheduling and monitoring
- âœ… Automatic retries on failure
- âœ… Visual pipeline monitoring
- âœ… Can run tasks in parallel
- âœ… Production-ready

**Disadvantages:**
- âŒ Requires Docker
- âŒ More complex setup
- âŒ Slower initial startup

---

## Project Structure

```
nfl-bdb-26-analytics/
â”œâ”€â”€ run_it.py                   # Pipeline orchestrator
â”œâ”€â”€ docker-compose.yml          # Docker services config
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”‚
â”œâ”€â”€ data/                       # Input data (not tracked)
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ 2023_input_all.parquet
â”‚   â”œâ”€â”€ supplementary_data.csv
â”‚   â””â”€â”€ sumer_bdb/
â”‚       â”œâ”€â”€ sumer_coverages_player_play.parquet
â”‚       â””â”€â”€ sumer_coverages_frame.parquet
â”‚
â”œâ”€â”€ outputs/                    # Generated data
â”‚   â”œâ”€â”€ dataframe_a/
â”‚   â”‚   â””â”€â”€ v2.parquet
â”‚   â”œâ”€â”€ dataframe_b/
â”‚   â”‚   â”œâ”€â”€ v3.parquet
â”‚   â”‚   â””â”€â”€ v3_pilot_3games.parquet
â”‚   â”œâ”€â”€ dataframe_c/
â”‚   â”‚   â”œâ”€â”€ v3.parquet
â”‚   â”‚   â””â”€â”€ v3_pilot_3games.parquet
â”‚   â””â”€â”€ dataframe_d/
â”‚       â””â”€â”€ v1.parquet
â”‚
â”œâ”€â”€ dags/                       # Airflow DAGs (copy run_it.py here)
â”œâ”€â”€ logs/                       # Airflow logs
â””â”€â”€ plugins/                    # Airflow plugins
```

---

## Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check Prerequisites â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataframe A (v2)  â”‚  Node-level features per frame
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                    â”‚
           â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataframe B (v3)  â”‚  â”‚   Dataframe D (v2)  â”‚
â”‚  Play-level + Ball  â”‚  â”‚   Frame-level       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                        â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataframe C (v3)  â”‚  Edge-level + Ball trajectory
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Summary Report     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dependencies:**
- Dataframe B depends on A (needs passer position)
- Dataframe C depends on A and B (needs both node + play features)
- Dataframe D is independent (can run parallel to B)

---

## Pilot Mode

Both dataframe_b_v3.py and dataframe_c_v3.py support **PILOT_MODE**:

```python
# At top of each script
PILOT_MODE = True   # Process only 3 games
PILOT_N_GAMES = 3
```

**When to use:**
- âœ… Development and testing
- âœ… Quick pipeline validation
- âœ… Attention model prototyping

**When to disable:**
- âœ… Final submission run
- âœ… Full model training
- âœ… Production deployment

---

## Troubleshooting

### Docker Issues

**Problem:** Port 8080 already in use
```bash
# Change port in docker-compose.yml
ports:
  - "8081:8080"  # Use 8081 instead
```

**Problem:** Permission denied
```bash
# Set correct AIRFLOW_UID
id -u  # Get your user ID
# Update .env file with your UID
```

**Problem:** Container won't start
```bash
# Check logs
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler

# Restart services
docker-compose restart
```

### Standalone Mode Issues

**Problem:** Module not found
```bash
# Ensure you're in project root
pip install -r requirements.txt
```

**Problem:** File not found
```bash
# Verify data structure
ls data/train/
ls data/sumer_bdb/
```

**Problem:** Script fails
```bash
# Run individual scripts to isolate issue
python dataframe_a_v2.py
python dataframe_b_v3.py
# etc.
```

---

## Next Steps

### After Pipeline Completes:

1. **Verify outputs**
   ```bash
   ls -lh outputs/dataframe_*/
   ```

2. **Check summary report** (printed at end)

3. **Begin model development**
   - Load pilot data for quick iteration
   - Build attention mechanism
   - Test with 3-game subset

4. **Full production run**
   - Set PILOT_MODE = False
   - Run complete pipeline
   - Train final model

---

## Performance Tips

### For Faster Processing:

1. **Use Pilot Mode** during development (3 games vs 270+)
2. **Parallel execution** with Airflow (B and D run together)
3. **Chunked processing** in scripts (already implemented)
4. **Monitor memory** usage during large runs

### Expected Times (Full Dataset):

- Dataframe A: ~30-60 minutes
- Dataframe B: ~5-10 minutes  
- Dataframe C: ~2-3 hours (most intensive)
- Dataframe D: ~2-5 minutes

### Expected Times (Pilot Mode - 3 games):

- Dataframe A: ~2-5 minutes
- Dataframe B: ~1 minute
- Dataframe C: ~5-10 minutes
- Dataframe D: ~1 minute

---

## Support

For issues with:
- **Pipeline scripts**: Check individual script documentation
- **Airflow setup**: See Airflow docs (airflow.apache.org)
- **Docker issues**: See Docker docs (docs.docker.com)

**Deadline: December 16, 2025**

Good luck with your Big Data Bowl submission! ğŸˆ
